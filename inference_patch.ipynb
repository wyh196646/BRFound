{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04dc924d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Direct imports successful\n",
      "Using device: cuda\n",
      "Extracted features shape: (1, 768)\n",
      "Extracted features shape: (1, 768)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from easydict import EasyDict\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Change to the correct directory\n",
    "os.chdir('/home/yuhaowang/project/BRFound')\n",
    "\n",
    "# Add current directory to Python path\n",
    "sys.path.insert(0, '/home/yuhaowang/project/BRFound')\n",
    "\n",
    "# Direct imports to avoid module issues\n",
    "try:\n",
    "    from src.vision_transformer import vit_base, DinoVisionTransformer\n",
    "    from src.utils import load_pretrained_weights\n",
    "    print(\"✓ Direct imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "\n",
    "def build_model_from_cfg(cfg, only_teacher=False):\n",
    "    \"\"\"Build model from configuration\"\"\"\n",
    "    args = cfg.student\n",
    "    img_size = cfg.crops.global_crops_size\n",
    "    \n",
    "    if \"vit\" in args.arch:\n",
    "        vit_kwargs = dict(\n",
    "            img_size=img_size,\n",
    "            patch_size=args.patch_size,\n",
    "            init_values=args.layerscale,\n",
    "            ffn_layer=args.ffn_layer,\n",
    "            block_chunks=args.block_chunks,\n",
    "            qkv_bias=args.qkv_bias,\n",
    "            proj_bias=args.proj_bias,\n",
    "            ffn_bias=args.ffn_bias,\n",
    "            num_register_tokens=args.num_register_tokens,\n",
    "            interpolate_offset=args.interpolate_offset,\n",
    "            interpolate_antialias=args.interpolate_antialias,\n",
    "        )\n",
    "        \n",
    "        if args.arch == 'vit_base':\n",
    "            teacher = vit_base(**vit_kwargs)\n",
    "        else:\n",
    "            # Fallback to DinoVisionTransformer constructor\n",
    "            teacher = DinoVisionTransformer(**vit_kwargs)\n",
    "            \n",
    "        if only_teacher:\n",
    "            return teacher, teacher.embed_dim\n",
    "            \n",
    "        # For student model, add dropout parameters\n",
    "        if args.arch == 'vit_base':\n",
    "            student = vit_base(\n",
    "                **vit_kwargs,\n",
    "                drop_path_rate=args.drop_path_rate,\n",
    "                drop_path_uniform=args.drop_path_uniform,\n",
    "            )\n",
    "        else:\n",
    "            student = DinoVisionTransformer(\n",
    "                **vit_kwargs,\n",
    "                drop_path_rate=args.drop_path_rate,\n",
    "                drop_path_uniform=args.drop_path_uniform,\n",
    "            )\n",
    "        embed_dim = student.embed_dim\n",
    "        \n",
    "        return student, teacher, embed_dim\n",
    "\n",
    "def load_pretrained_model(weights_path, device='cuda'):\n",
    "    model = vit_base(patch_size=16)\n",
    "    state_dict = torch.load(weights_path, map_location=device)\n",
    "\n",
    "    # Remove potential prefixes in keys\n",
    "    state_dict = {k.replace(\"module.\", \"\").replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    #check if load successfully\n",
    "    if not all(k in model.state_dict() for k in state_dict.keys()):\n",
    "        missing_keys = set(state_dict.keys()) - set(model.state_dict().keys())\n",
    "        print(f\"Missing keys in the model state dict: {missing_keys}\")\n",
    "    else:\n",
    "        print(\"All keys loaded successfully.\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "\n",
    "def extract_features(image_path, model, device='cuda'):\n",
    "    transform = get_transform()\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = model(image_tensor)\n",
    "\n",
    "    return features.cpu().numpy()\n",
    "\n",
    "def build_model_for_eval(config, pretrained_weights):\n",
    "    model, _ = build_model_from_cfg(config, only_teacher=True)\n",
    "    load_pretrained_weights(model, pretrained_weights, \"teacher\")\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    config = EasyDict({\n",
    "        'student': EasyDict({\n",
    "            'arch': 'vit_base', \n",
    "            'patch_size': 16, \n",
    "            'drop_path_rate': 0.3,  \n",
    "            'layerscale': 1.0e-05,  \n",
    "            'drop_path_uniform': True,  \n",
    "            'pretrained_weights': '',  \n",
    "            'ffn_layer': 'mlp',  \n",
    "            'block_chunks': 4,  \n",
    "            'qkv_bias': True,  \n",
    "            'proj_bias': True,  \n",
    "            'ffn_bias': True,  \n",
    "            'num_register_tokens': 0,  \n",
    "            'interpolate_antialias': False,  \n",
    "            'interpolate_offset': 0.1  \n",
    "        }),\n",
    "        'crops': EasyDict({\n",
    "            'global_crops_size': 224,\n",
    "        })    \n",
    "    })\n",
    "        \n",
    "    weights_path = './weights/patch_encoder.pth'\n",
    "    image_path = './images/patch_1.png'\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(weights_path):\n",
    "        print(f\"Warning: weights file not found at {weights_path}\")\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Warning: image file not found at {image_path}\")\n",
    "    \n",
    "    # Only proceed if imports were successful and files exist\n",
    "    try:\n",
    "        if os.path.exists(weights_path) and os.path.exists(image_path):\n",
    "            model = build_model_for_eval(config, weights_path)\n",
    "            features = extract_features(image_path, model, device=device)\n",
    "            print(f\"Extracted features shape: {features.shape}\")\n",
    "        else:\n",
    "            print(\"Skipping model evaluation due to missing files\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UNI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
